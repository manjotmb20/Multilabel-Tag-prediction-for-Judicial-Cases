# -*- coding: utf-8 -*-
"""assignmenttext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/manjotmb20/Judicial-Decision-Prediction/blob/master/assignmenttext.ipynb
"""

import numpy as np
import pandas as pd
import os
import zipfile
import matplotlib.pyplot as plt
import seaborn as sns

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
nltk.download('wordnet')

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import CountVectorizer
import operator
import re

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from keras.layers import Embedding,Conv1D,GlobalMaxPool1D,Dense,Activation,Dropout
from keras.models import Sequential

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

!pip install tokenizer

zipref=zipfile.ZipFile("Train_tags.zip")
zipref.extractall()
zipref.close()
zipref=zipfile.ZipFile("Train_docs.zip")
zipref.extractall()
zipref.close()

"""Loading doc file names and tag names in a list"""

list1=[]
col=[]
tags=[]
for i in range(80):
  list1.append("case_{}_statement.txt".format(i))
  col.append(i)
  tags.append("case{}.txt".format(i))

"""function to clean text data like lower casing every word , removing "\,'" and only allowing alphabets"""

def clean_text1(text):
  text=re.sub("\'", "", text )
  text=re.sub("[^a-zA-z]"," ",text)
  text=' '.join(text.split())
  text=text.lower()
  return text

"""Reading train docs and tags in lists"""

listtag=[]
for row in tags:
  tag=open("../content/Train_tags/"+row).read()
  listtag.append(tag.split(","))

content=[]
for name in df.name:
  s=open("../content/Train_docs/"+name,"r",encoding = "ISO-8859-1").read()
  content.append(clean_text1(s))

"""Creating a dataframe of the docs and tags after preprocessing"""

df=pd.DataFrame()
df["name"]=list1
df["content"]=content
df["tags"]=listtag

df

"""When we plot the frequency graph of words in docs we see that there are a lot of stopwords which we must remove"""

def freq_words(x, terms = 30): 
  all_words = ' '.join([text for text in x]) 
  all_words = all_words.split() 
  fdist = nltk.FreqDist(all_words) 
  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) 
  
  
  d = words_df.nlargest(columns="count", n = terms) 
  
 
  plt.figure(figsize=(12,15)) 
  ax = sns.barplot(data=d, x= "count", y = "word") 
  ax.set(ylabel = 'Word') 
  plt.show()
  

freq_words(df["content"], 100)

"""Removing Stopwords"""

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
def remove_stopwords(text):
  no_stopwords=[w for w in text.split() if not w in stop_words]
  return ' '.join(no_stopwords)
df["content"]=df["content"].apply(lambda x: remove_stopwords(x))

def freq_words(x, terms = 30): 
  all_words = ' '.join([text for text in x]) 
  all_words = all_words.split() 
  fdist = nltk.FreqDist(all_words) 
  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) 
  
  
  d = words_df.nlargest(columns="count", n = terms) 
  
 
  plt.figure(figsize=(12,15)) 
  ax = sns.barplot(data=d, x= "count", y = "word") 
  ax.set(ylabel = 'Word') 
  plt.show()
  

freq_words(df["content"], 100)

"""Using MultiLabelBinarizer as we need MultiLabel Output from model"""

from sklearn.preprocessing import MultiLabelBinarizer
multilabel_binarizer=MultiLabelBinarizer()
multilabel_binarizer.fit(df['tags'])
labels=multilabel_binarizer.classes_
y=multilabel_binarizer.transform(df["tags"])

"""Tfidf Vectorizer for converting docs to vector representation"""

tfidf_vectorizer=TfidfVectorizer(max_df=0.8,max_features=5000)
xtrain, xval, ytrain, yval = train_test_split(df['content'], y, test_size=0.2, random_state=9)

xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)
xval_tfidf = tfidf_vectorizer.transform(xval)

"""As we have a very low tag distribution, so we will use predict_proba instead of pred as 'pred' will always return 0 as it is a binary classification task so it will always give 0 as output due to low tag distribution across dataset"""

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import f1_score
lr = LogisticRegression()
clf = OneVsRestClassifier(lr)
clf.fit(xtrain_tfidf, ytrain)
yt=clf.predict_proba(xval_tfidf)

threshold= 0.15
y_pred_new = (yt >= threshold).astype(int)

"""Inverse Transform to convert labels to Tag name"""

results=multilabel_binarizer.inverse_transform(y_pred_new)

results

f1_score(yval, y_pred_new, average="micro")

"""Now loading the test data set into the model"""

zipref=zipfile.ZipFile("Test_docs.zip")
zipref.extractall()
zipref.close()

testlist=[]
for i in range(100,300):
  testlist.append("case_{}_statement.txt".format(i))

dftest=pd.DataFrame()
dftest["name"]=testlist

testcontent=[]
for name in dftest.name:
  s=open("../content/Test_docs/"+name,"r",encoding = "ISO-8859-1").read()
  testcontent.append(clean_text1(s))

dftest["content"]=testcontent

dftest.head()

dftest["content"]=dftest["content"].apply(lambda x: remove_stopwords(x))

def freq_words(x, terms = 30): 
  all_words = ' '.join([text for text in x]) 
  all_words = all_words.split() 
  fdist = nltk.FreqDist(all_words) 
  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) 
  
  
  d = words_df.nlargest(columns="count", n = terms) 
  
 
  plt.figure(figsize=(12,15)) 
  ax = sns.barplot(data=d, x= "count", y = "word") 
  ax.set(ylabel = 'Word') 
  plt.show()
  

freq_words(dftest["content"], 100)

xtest_tfidf = tfidf_vectorizer.transform(dftest["content"])

ytest=clf.predict_proba(xtest_tfidf)
threshold= 0.16
ytest_pred= (ytest>= threshold).astype(int)
results=multilabel_binarizer.inverse_transform(ytest_pred)

dftest["predicted tags"]=results

dftest.to_csv('predicted_tags.csv')

